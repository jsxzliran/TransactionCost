{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Data_generator_multiple as dg\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import math, random\n",
    "import matplotlib.pyplot as plt\n",
    "import UtilityLoss\n",
    "from sklearn import *\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize seed, mu, sigma, S0, paths, steps, T\n",
    "torch.cuda.empty_cache()\n",
    "seed = 36\n",
    "num_stocks = 10\n",
    "npaths = 40000\n",
    "seq_length = 120\n",
    "T=10\n",
    "np.random.seed(seed)\n",
    "s0=5*np.ones(num_stocks)\n",
    "mu = np.random.uniform(0.03, 0.12,(num_stocks))\n",
    "# Generate a covariance matrix as the Sigma\n",
    "cov = datasets.make_sparse_spd_matrix(dim=num_stocks, alpha=0.5, norm_diag=False, smallest_coef=0.1, \\\n",
    "    largest_coef=0.5, random_state=None)\n",
    "# Adjust the covariace matrix to be in reasonable value\n",
    "cov = 0.2*cov\n",
    "#cov = np.diag(np.diag(cov))\n",
    "trade_cost = 0.03\n",
    "utility_gamma = 3.0001\n",
    "is_one_liquid = False\n",
    "\n",
    "# Notice the single is used only for plotting a 2 asset situation\n",
    "if (num_stocks==2):\n",
    "    rho = np.round(cov[0,1]/(np.sqrt(cov[0,0]*cov[1,1])),3)\n",
    "np.savetxt(\"cov_seed{}.csv\".format(seed), np.round(cov,3), delimiter=\",\")\n",
    "np.savetxt(\"mu_seed{}.csv\".format(seed), np.round(mu,4), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.36475423, 0.43105003, 0.89954021, 0.40323886, 0.57692084,\n",
       "       0.74477089, 0.38883343, 1.13914717, 0.71096557, 0.29835995])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To determine optimal trading frequency, based on Johannes Muhle-Karbe's paper, higher dimension waited to be verified\n",
    "# step 1: Calculting the Markowitz optimal\n",
    "Markowitz_opt = np.matmul(np.linalg.inv(cov),mu)/utility_gamma\n",
    "Markowitz_opt_tensor =  torch.tensor(Markowitz_opt,dtype = torch.float).to(device)\n",
    "# step 2: Get the sigma where cov=sigma*sigma.transpose()\n",
    "sigma = np.linalg.cholesky(cov)\n",
    "# step 3: Calculate beta and the norm of beta through step 2\n",
    "beta = -(sigma-np.matmul(Markowitz_opt,sigma))*np.transpose(Markowitz_opt)\n",
    "beta_n = np.linalg.norm(beta)\n",
    "# step 4: Calculate the optimal trading frequency\n",
    "trade_f = math.pow(trade_cost*np.sqrt(2/np.pi)*beta_n/(utility_gamma/2*np.trace(np.matmul(np.matmul(beta.transpose(),sigma),beta))),2/3)\n",
    "#seq_length = int(T/trade_f)\n",
    "seq_length\n",
    "Markowitz_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.36575423 0.43205003 0.90054021 0.40423886 0.57792084 0.74577089\n",
      " 0.38983343 1.14014717 0.71196557 0.29935995]\n",
      "[0.36375423 0.43005003 0.89854021 0.40223886 0.57592084 0.74377089\n",
      " 0.38783343 1.13814717 0.70996557 0.29735995]\n"
     ]
    }
   ],
   "source": [
    "# Define the distance for initial non trade region\n",
    "delta = 0.1*np.power(np.diag(cov)*trade_cost+0.001,2/3)\n",
    "delta_tensor = torch.tensor(delta, dtype = torch.float)\n",
    "print(Markowitz_opt+delta)\n",
    "print(Markowitz_opt-delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a stock simulation with prices, returns\n",
    "stock = dg.ManyStocks(seed,num_stocks,mu,s0,cov,npaths,seq_length-1,T)\n",
    "returns = torch.tensor(stock.Returns(),dtype=torch.float).to(device)\n",
    "# Create a default strategy as initial input, better use the optimal strategy without cost\n",
    "#strategy = -1*torch.ones((num_stocks,seq_length,npaths),dtype=torch.float).to(device)\n",
    "strategy = (torch.tensor(Markowitz_opt,dtype=torch.float).view(num_stocks,1,1)*torch.ones((num_stocks,seq_length,npaths),dtype=torch.float)).to(device)\n",
    "# Create a trading cost\n",
    "cost =  torch.tensor(trade_cost*np.ones([num_stocks,seq_length-1,npaths]),dtype=torch.double).to(device)\n",
    "#cost[0,:,:]=0.00\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To determine the theoretical no trade region of 2 assets, one of which is liquid, to compare with RNN results\n",
    "# the first asset is liquid and the second illiquid\n",
    "if ((num_stocks==2) & (is_one_liquid==True)):\n",
    "    rho = np.round(cov[0,1]/(np.sqrt(cov[0,0]*cov[1,1])),3)\n",
    "    beta1 = np.sqrt(np.prod(np.diag(cov)))*rho/np.diag(cov)[1]\n",
    "    beta2 = np.sqrt(np.prod(np.diag(cov)))*rho/np.diag(cov)[0]\n",
    "    sigma_I = np.sqrt(np.diag(cov)[1]*(1-rho*rho))\n",
    "    # Same as the Markowitz_optimal\n",
    "    pi1 = (mu[0]-beta1*mu[1])/(utility_gamma*(1-rho*rho)*np.diag(cov)[0])\n",
    "    pi2 = (mu[1]-beta2*mu[0])/(utility_gamma*(1-rho*rho)*np.diag(cov)[1])\n",
    "    # lambda used to define the width of no trade region\n",
    "    lamb = np.power(0.75*np.power(utility_gamma,2)*np.power(sigma_I,4)*pi2*pi2*(\\\n",
    "            np.power(beta2-mu[0]/(utility_gamma*np.diag(cov)[0]),2)*np.diag(cov)[0]\\\n",
    "            + np.power(1-pi2,2)*np.power(sigma_I,2)),1/3)*np.power(trade_cost,1/3)\n",
    "    pi1_p = pi1+beta2*lamb/(utility_gamma*np.power(sigma_I,2))\n",
    "    pi1_n = pi1-beta2*lamb/(utility_gamma*np.power(sigma_I,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate return of specific strategy\n",
    "def cal_return(strat):   \n",
    "    r0 = torch.sum(strat[:,:-1, :]*returns,0)\n",
    "    r1 = r0-torch.sum(cost*abs((r0+1)*strat[:,1:, :]-(returns+1)*strat[:,:-1, :]),axis=0)\n",
    "    r2 = r0-torch.sum(cost*abs((r1+1)*strat[:,1:, :]-(returns+1)*strat[:,:-1, :]),axis=0)\n",
    "    r3 = r0-torch.sum(cost*abs((r2+1)*strat[:,1:, :]-(returns+1)*strat[:,:-1, :]),axis=0)\n",
    "    r4 = r0-torch.sum(cost*abs((r3+1)*strat[:,1:, :]-(returns+1)*strat[:,:-1, :]),axis=0)\n",
    "    return r4\n",
    "\n",
    "class Cal_return(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, strat):\n",
    "\n",
    "        return cal_return(strat) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a log utility function\n",
    "class LogUtilityLoss(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LogUtilityLoss,self).__init__()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        loss = -torch.mean(torch.log(x))\n",
    "        return loss\n",
    "\n",
    "# Define a power utility function\n",
    "class PowerUtilityLoss(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self,gamma):\n",
    "        super(PowerUtilityLoss,self).__init__()\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self,x):\n",
    "        loss = -torch.mean((torch.pow(x,1-self.gamma)-1)/(1-self.gamma))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize a RNN layer with double relu for multiple assets\n",
    "# considering returns data to build a changed strategy weight according to price change\n",
    "class MyRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, batch_size,dim_size):\n",
    "        \"\"\"Initialize params.\"\"\"\n",
    "        super(MyRNN, self).__init__()\n",
    "        # read input parameters\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.dim_size = dim_size\n",
    "\n",
    "        self.input_param = nn.Linear(input_size, dim_size,  hidden_size).to(device)\n",
    "        self.hidden_param = nn.Linear(hidden_size,  dim_size, hidden_size).to(device)\n",
    "        self.fc1_param = nn.Linear(hidden_size, dim_size,hidden_size).to(device)\n",
    "        self.fc2_param = nn.Linear(hidden_size, dim_size, hidden_size).to(device)\n",
    "        # Define a rotate layer\n",
    "        #self.rotate_param1 = nn.Linear(1,1,bias=False)\n",
    "        #self.rotate_param2 = nn.Linear(1,1,bias=False)\n",
    "        self.rotate_param = nn.Linear(dim_size,dim_size,bias=False)\n",
    "        \n",
    "    # Forward function allows a form:\n",
    "    # h_t = w_fc2*relu(w_fc1*relu(w_inp*x_t+b_inp+w_h*h_{t-1}+b_h)+b_fc1)+b_fc2+b_fc1-b_h1\n",
    "    def forward(self, input, hidden):\n",
    "        \n",
    "        def recurrence(input, hidden):\n",
    "            # w_inp*x_t+b_inp+w_h*h_{t-1}+b_h\n",
    "            ingate = self.input_param.weight.view(self.dim_size,self.input_size,self.hidden_size)*input \\\n",
    "                    + self.hidden_param.weight.view(self.dim_size,self.hidden_size,self.hidden_size)*hidden \\\n",
    "                    - Markowitz_opt_tensor.view(self.dim_size,self.input_size,self.hidden_size) \\\n",
    "                    + self.hidden_param.bias.view(self.dim_size,self.hidden_size,self.hidden_size)\n",
    "            # w_fc1*relu(ingate)+b_fc1\n",
    "            ingate2 = self.fc1_param.weight.view(self.dim_size,self.hidden_size,self.hidden_size)*F.relu(ingate)\\\n",
    "                    + 2*self.hidden_param.bias.view(self.dim_size,self.hidden_size,self.hidden_size)\n",
    "            # w_fc2*relu(ingate2)+b_fc2+b_fc1-b_h1\n",
    "            h       = self.fc2_param.weight.view(self.dim_size,self.hidden_size,self.hidden_size)*F.relu(ingate2)\\\n",
    "                    + Markowitz_opt_tensor.view(self.dim_size,self.input_size,self.hidden_size) \\\n",
    "                    + self.hidden_param.bias.view(self.dim_size,self.hidden_size,self.hidden_size)\n",
    "            return h\n",
    "        # Define the rotate function\n",
    "        # def myrotate(input):\n",
    "        #     input_new = input.squeeze(1)\n",
    "        #     out = self.rotate_param.weight@input_new\n",
    "        #     return out.unsqueeze(1)\n",
    "\n",
    "        def myrotate(input_rotate):\n",
    "            input_new = input_rotate.squeeze(1)\n",
    "            out = torch.matmul(self.rotate_param.weight,input_new)\n",
    "            #out1 = self.rotate_param1.weight*input_new[1,:]+input_new[0,:]\n",
    "            #out2 = -self.rotate_param1.weight*input_new[0,:]+input_new[1,:]\n",
    "            #out = torch.stack((out1,out2),0)\n",
    "            return out.unsqueeze(1)\n",
    "\n",
    "        output = []\n",
    "        steps = range(input.size(1))\n",
    "        myret = returns\n",
    "        for i in steps:\n",
    "            if i ==0:\n",
    "                hidden = input[:,0,:].view(self.dim_size,1,self.batch_size).to(device)\n",
    "                #hidden = (torch.tensor(Markowitz_opt,dtype=torch.float).view(self.dim_size,1,1)*torch.ones((self.dim_size,1,self.batch_size),dtype=torch.float)).to(device)\n",
    "            else:\n",
    "                # pi_t = myrotate(pi_{t-1}*(1+r_t)/(1+sum(pi_{t-1}*r_t))) due to change of price after rebalance\n",
    "                adjust_pi = myrotate(\\\n",
    "                                    hidden.view(self.dim_size,1,self.batch_size)*(1+myret[:,i-1,:].view(self.dim_size,1,self.batch_size))\\\n",
    "                                    /(1+torch.sum(hidden.view(self.dim_size,1,self.batch_size)*myret[:,i-1,:].view(self.dim_size,1,\\\n",
    "                                    self.batch_size),0))\\\n",
    "                                    )\n",
    "\n",
    "                hidden = myrotate(recurrence(input[:,i,:].view(self.dim_size,self.input_size,self.batch_size), adjust_pi))\n",
    "                \n",
    "            output.append(hidden)\n",
    "\n",
    "        output = torch.cat(output, 1)\n",
    "\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers, batch_size, seq_length, dim_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "        self.dim_size = dim_size\n",
    "        # the rnn layer which works as out, hidden_t = f(out_(t), hidden_(t-1)), used to approximate pi^*_(t)= f(pi^*_(t-1),pi_t)\n",
    "        self.rnn = MyRNN(input_size, hidden_size, batch_size, dim_size).to(device)\n",
    "        self.out = nn.Linear(dim_size, hidden_size,bias=False).to(device)\n",
    "        # initialize some bias and weight\n",
    "        self.rnn.input_param.weight = torch.nn.Parameter(torch.zeros_like(self.rnn.input_param.weight))\n",
    "        self.rnn.hidden_param.weight = torch.nn.Parameter(torch.ones_like(self.rnn.hidden_param.weight))\n",
    "        self.rnn.hidden_param.bias = torch.nn.Parameter(delta_tensor)\n",
    "        self.rnn.fc1_param.bias = torch.nn.Parameter(2*delta_tensor)\n",
    "        #self.rnn.hidden_param.bias = torch.nn.Parameter(-0.0*torch.ones_like(self.rnn.hidden_param.bias))\n",
    "        #self.rnn.fc1_param.bias = torch.nn.Parameter(0.1*torch.ones_like(self.rnn.fc1_param.bias))\n",
    "        self.rnn.fc1_param.weight = torch.nn.Parameter(-1*torch.ones_like(self.rnn.fc1_param.weight))\n",
    "        self.rnn.fc2_param.weight = torch.nn.Parameter(-1*torch.ones_like(self.rnn.fc2_param.weight))\n",
    "        #self.rnn.rotate_param1.weight = torch.nn.Parameter(torch.zeros(1))\n",
    "        #self.rnn.rotate_param2.weight = torch.nn.Parameter(torch.zeros(1))\n",
    "        self.rnn.rotate_param.weight = torch.nn.Parameter(torch.diag(torch.ones(self.dim_size)))\n",
    "        #self.rnn.rotate_param.weight = torch.nn.Parameter(10*torch.rand((2,2)))\n",
    "        self.out.weight = torch.nn.Parameter(*torch.ones_like(self.out.weight))\n",
    "\n",
    "    def step(self, input, hidden=None):\n",
    "        output, hidden = self.rnn(input, hidden).to(device)\n",
    "        output2 = self.out.weight.view(dim_size,hidden_size,hidden_size)*output\n",
    "        return output, output2\n",
    "\n",
    "    def forward(self, inputs, hidden=None):\n",
    "        hidden = self.__init__hidden().to(device)\n",
    "        output, hidden = self.rnn(inputs.float(), hidden.float())\n",
    "        # output2 the overall wealth at time T\n",
    "        output2 = torch.prod(cal_return(output).to(device)+1,0)\n",
    "        return  output, output2\n",
    "        #return  output\n",
    "\n",
    "    def __init__hidden(self):\n",
    "        hidden = 0.0*torch.ones(self.dim_size, self.hidden_size,  self.batch_size,dtype=torch.float64).to(device)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model parameters, using paths as bath size, might not be the best, will try to fix later\n",
    "n_epochs = 60\n",
    "input_size = 1\n",
    "hidden_size = 1\n",
    "n_layers = 1\n",
    "batch_size = npaths\n",
    "seq_length = seq_length\n",
    "dim_size = num_stocks\n",
    "\n",
    "model = SimpleRNN(input_size, hidden_size, n_layers, batch_size, seq_length,dim_size).to(device)\n",
    "#criterion = LogUtilityLoss()\n",
    "criterion = PowerUtilityLoss(utility_gamma)\n",
    "#model.rnn.bias_ih_l0.requires_grad = False\n",
    "#model.rnn.bias_ih_l1.requires_grad = False\n",
    "#model.rnn.weight_hh_l0.requires_grad = False\n",
    "model.out.weight.requires_grad = False\n",
    "model.rnn.input_param.weight.requires_grad = False\n",
    "model.rnn.hidden_param.weight.requires_grad = False\n",
    "model.rnn.fc1_param.weight.requires_grad = False\n",
    "model.rnn.fc2_param.weight.requires_grad = False\n",
    "model.rnn.fc1_param.bias.requires_grad = False\n",
    "model.rnn.fc2_param.bias.requires_grad = False\n",
    "#model.rnn.fc2_param.bias.requires_grad = True\n",
    "optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "#model.rnn.rotate_param.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(-0.4898, device='cuda:0', dtype=torch.float64, grad_fn=<NegBackward>)\n",
      "10 tensor(-0.4898, device='cuda:0', dtype=torch.float64, grad_fn=<NegBackward>)\n",
      "20 tensor(-0.4898, device='cuda:0', dtype=torch.float64, grad_fn=<NegBackward>)\n",
      "30 tensor(-0.4899, device='cuda:0', dtype=torch.float64, grad_fn=<NegBackward>)\n",
      "40 tensor(-0.4899, device='cuda:0', dtype=torch.float64, grad_fn=<NegBackward>)\n",
      "50 tensor(-0.4899, device='cuda:0', dtype=torch.float64, grad_fn=<NegBackward>)\n",
      "60 tensor(-0.4900, device='cuda:0', dtype=torch.float64, grad_fn=<NegBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Loss for plotting\n",
    "losses = np.zeros(n_epochs+1) \n",
    "\n",
    "for epoch in range(n_epochs+1):\n",
    "    inputs = strategy.to(device)\n",
    "    fina_strat, outputs = model(inputs.double(), None)\n",
    "\n",
    "    loss = criterion(outputs)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    losses[epoch] += loss\n",
    "    if epoch % 10 == 0:\n",
    "        print(epoch, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot loss curve\n",
    "epochs = range(n_epochs+1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(epochs, np.round(losses,3))\n",
    "\n",
    "ax.set(xlabel='Epochs', ylabel='Loss',\n",
    "       title='Loss')\n",
    "ax.grid()\n",
    "fig.savefig(\"loss_stocks{}_cost{}.png\".format(num_stocks,trade_cost))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn.input_param.weight tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0')\n",
      "rnn.input_param.bias tensor([-0.6853,  0.1903,  0.7432,  0.5396, -0.2504, -0.5626, -0.1919, -0.9124,\n",
      "         0.0560, -0.5426], device='cuda:0')\n",
      "rnn.hidden_param.weight tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]], device='cuda:0')\n",
      "rnn.hidden_param.bias tensor([0.0013, 0.0013, 0.0009, 0.0007, 0.0010, 0.0005, 0.0007, 0.0012, 0.0015,\n",
      "        0.0011], device='cuda:0')\n",
      "rnn.fc1_param.weight tensor([[-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.]], device='cuda:0')\n",
      "rnn.fc1_param.bias tensor([0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0020,\n",
      "        0.0020], device='cuda:0')\n",
      "rnn.fc2_param.weight tensor([[-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.]], device='cuda:0')\n",
      "rnn.fc2_param.bias tensor([-0.8886,  0.9823, -0.9008, -0.4955, -0.4724, -0.3284,  0.0859, -0.5634,\n",
      "        -0.6561,  0.5868], device='cuda:0')\n",
      "rnn.rotate_param.weight tensor([[ 1.0003e+00,  3.7301e-04,  7.7394e-04,  3.4713e-04,  4.9989e-04,\n",
      "          6.4166e-04,  3.3817e-04,  9.8282e-04,  6.1360e-04,  2.5913e-04],\n",
      "        [ 1.7030e-05,  1.0000e+00,  3.6080e-05,  1.7634e-05,  2.4451e-05,\n",
      "          3.3847e-05,  1.7417e-05,  5.7776e-05,  2.8290e-05,  1.3376e-05],\n",
      "        [-3.2593e-04, -3.8308e-04,  9.9920e-01, -3.5845e-04, -5.1294e-04,\n",
      "         -6.6243e-04, -3.4528e-04, -1.0100e-03, -6.3060e-04, -2.6580e-04],\n",
      "        [-1.9346e-04, -2.2313e-04, -4.7157e-04,  9.9979e-01, -3.0558e-04,\n",
      "         -3.9176e-04, -2.0570e-04, -6.0189e-04, -3.7356e-04, -1.5755e-04],\n",
      "        [ 1.1780e-04,  1.3884e-04,  2.9133e-04,  1.3082e-04,  1.0002e+00,\n",
      "          2.4029e-04,  1.2425e-04,  3.6613e-04,  2.3033e-04,  9.6849e-05],\n",
      "        [-2.6118e-05, -2.9718e-05, -6.2673e-05, -2.8195e-05, -4.0465e-05,\n",
      "          9.9995e-01, -2.7989e-05, -7.6526e-05, -4.9491e-05, -2.1565e-05],\n",
      "        [-4.6904e-05, -5.9433e-05, -1.2614e-04, -5.6002e-05, -7.4369e-05,\n",
      "         -9.9443e-05,  9.9995e-01, -1.5262e-04, -9.6982e-05, -3.8886e-05],\n",
      "        [-1.9239e-04, -2.2761e-04, -4.7516e-04, -2.1288e-04, -3.0464e-04,\n",
      "         -3.9337e-04, -2.0531e-04,  9.9940e-01, -3.7587e-04, -1.5720e-04],\n",
      "        [ 8.8843e-05,  1.0485e-04,  2.1937e-04,  9.8416e-05,  1.4091e-04,\n",
      "          1.8193e-04,  9.5258e-05,  2.7826e-04,  1.0002e+00,  7.3016e-05],\n",
      "        [-1.8603e-04, -2.2114e-04, -4.6038e-04, -2.0926e-04, -2.9658e-04,\n",
      "         -3.8231e-04, -2.0203e-04, -5.8702e-04, -3.7099e-04,  9.9985e-01]],\n",
      "       device='cuda:0')\n",
      "out.weight tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# output the weights and bias of the model parameter\n",
    "for name, param in model.named_parameters():\n",
    "     print (name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.4898, device='cuda:0', dtype=torch.float64)\n",
      "tensor(-0.4900, device='cuda:0', dtype=torch.float64, grad_fn=<NegBackward>)\n",
      "tensor([ 20.6084,  15.4779, 128.6806,  ...,  17.0959,  17.6102,  38.7738],\n",
      "       device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(criterion(torch.prod(cal_return(strategy)+1,0)))\n",
    "print(criterion(outputs))\n",
    "print(torch.prod(cal_return(strategy)+1,0))\n",
    "np.savetxt(\"buy.csv\", np.round(Markowitz_opt-model.rnn.hidden_param.bias.cpu().detach().numpy(),3), delimiter=\",\")\n",
    "np.savetxt(\"sell.csv\", np.round(Markowitz_opt+model.rnn.hidden_param.bias.cpu().detach().numpy(),3), delimiter=\",\")\n",
    "np.savetxt(\"merton.csv\", np.round(Markowitz_opt,3), delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c3ac46d4b3417523032261e4e785e5ad81a0fff735abedd01ccd625b28d394c1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('cost': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
